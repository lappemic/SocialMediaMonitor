{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now (05.05.2022) not further processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd, numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ChromeOptions\n",
    "#from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pathlib\n",
    "from datetime import timedelta\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# get the date of the moste recent article in the merged dataset\n",
    "liverss_and_coms_path = pathlib.Path.home() / 'SocialMediaMonitor' / 'data' / 'raw' / 'liverss_and_coms_20min.csv'\n",
    "liverss_and_coms_df = pd.read_csv(liverss_and_coms_path)\n",
    "liverss_and_coms_df['published'] = pd.to_datetime(liverss_and_coms_df['published'])\n",
    "most_recent_article = liverss_and_coms_df['published'].max()\n",
    "\n",
    "# get the ids of the articles that are less than 15 days old\n",
    "recent_date = pd.to_datetime(most_recent_article-timedelta(15), utc=True)\n",
    "mask = (liverss_and_coms_df['published'] > recent_date) # & (total_df['is_com_or_sub'] == 'sub')\n",
    "recent_articles = liverss_and_coms_df.loc[mask]\n",
    "recent_articles_id = recent_articles['id'].tolist()\n",
    "#print(recent_articles_id[:3])\n",
    "print(len(recent_articles_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Chrome webdriver\n",
    "driver = webdriver.Chrome('/users/michaellappert/Downloads/chromedriver') # Needs to be adjusted client independently\n",
    "\n",
    "# get the commenents of those articles\n",
    "new_coms_df = pd.DataFrame(columns = ['author', 'published', 'summary', 'is_com_or_sub', 'link'])\n",
    "\n",
    "for article_id in recent_articles_id:\n",
    "    # print(article)\n",
    "    url = 'https://www.20min.ch/comment/'\n",
    "    comments_url = url + str(article_id)\n",
    "    # print(comments_url)\n",
    "    driver.get(comments_url)\n",
    "    \n",
    "    # scroll to load entire page (uses lazy loading)\n",
    "    check_height = driver.execute_script('return document.body.scrollHeight;')\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        height = driver.execute_script(\"return document.body.scrollHeight;\") \n",
    "        if height == check_height: \n",
    "            break \n",
    "        check_height = height\n",
    "    \n",
    "    # get authors\n",
    "    authors = driver.find_elements_by_class_name('CommentCard_authorNickname__booTY')\n",
    "    # print(authors)\n",
    "    author_list = [author.get_attribute('innerHTML') for author in authors]\n",
    "    # print(author_list)\n",
    "\n",
    "    # get publication dates\n",
    "    published_date = driver.find_elements_by_class_name('CommentCard_createdAt__LxEL2')\n",
    "    # print(published_date)\n",
    "    date_list = [pd.to_datetime(date.get_attribute('innerHTML'), format = '%d.%m.%Y, %H:%M', utc = True) for date in published_date]\n",
    "    # print(len(date_list))\n",
    "    # print(date_list)\n",
    "    \n",
    "    # get text bodys\n",
    "    bodys = driver.find_elements_by_class_name(\"CommentCard_body__KWmXR\")\n",
    "    # print(bodys)\n",
    "    body_list = [body.get_attribute('innerHTML') for body in bodys]\n",
    "    # print(body_list)\n",
    "    \n",
    "    # assesrt to make sure that we have all the data for every com\n",
    "    # may fail if the webdriver didn't wait for the page to load\n",
    "    assert len(body_list) == len(date_list) == len(author_list), \"Assertion problem, probably due to page parsing before page is fully loaded\"\n",
    "    \n",
    "    # create a temporary dataframe with the data and write it to the new_coms_df\n",
    "    temp_df = pd.DataFrame(list(zip(author_list, date_list, body_list)), columns =['author', 'published', 'summary'])\n",
    "    # print(temp_df)\n",
    "    temp_df['id'] = article_id\n",
    "    # temp_df.head()\n",
    "    temp_df['is_com_or_sub'] = 'com'\n",
    "    # print(temp_df.head())\n",
    "    # time.sleep(2)\n",
    "    temp_df['link'] = comments_url\n",
    "    # print(temp_df.head())\n",
    "    # time.sleep(2)\n",
    "    # temp_df = temp_df[temp_df['published'] > most_recent_article] # -> GLOBAL VARIABLE FOR LAST_COM_CHECK MUST BE INCLUDED\n",
    "    print(f'Article {comments_url} has {len(temp_df)} comments')\n",
    "    if len(temp_df) >0:\n",
    "        new_coms_df = pd.concat([new_coms_df, temp_df], ignore_index=True, axis=0)\n",
    "\n",
    "# new_coms_df.head()\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c76eb199b8e1fb9fa01facb4084fceb7f49eeb0e37fb232cc5bedee7779b1e8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
