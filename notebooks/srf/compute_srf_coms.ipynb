{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from datetime import timedelta\n",
    "import pathlib\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute coms of newly aquired articles from srf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-08 18:35:00+00:00\n",
      "['https://www.srf.ch/sport/fussball/super-league/4-0-heimsieg-gegen-servette-luzern-mindestens-in-der-barrage-und-nur-noch-3-punkte-hinter-gc', 'https://www.srf.ch/sport/fussball/super-league/3-0-heimsieg-gegen-lausanne-luzern-feiert-wichtigen-erfolg-im-abstiegskampf', 'https://www.srf.ch/news/abstimmungen-15-mai-2022/abstimmung-stadt-luzern-braucht-luzern-mehr-zusammenhaengende-velorouten']\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# get the date of the moste recent article in the merged dataset\n",
    "liverss_and_coms_path = pathlib.Path.home() / 'Desktop' / 'SocialMediaMonitor' / 'data' / 'raw' / 'liverss_and_coms_srf.csv'\n",
    "liverss_and_coms_df = pd.read_csv(liverss_and_coms_path)\n",
    "# print(liverss_and_coms_df)\n",
    "# print(liverss_and_coms_df.columns)\n",
    "liverss_and_coms_df['published'] = pd.to_datetime(liverss_and_coms_df['published'], errors = 'coerce')\n",
    "most_recent_article = liverss_and_coms_df['published'].max()\n",
    "print(most_recent_article)\n",
    "\n",
    "# get the ids of the articles that are less than 15 days old\n",
    "recent_date = pd.to_datetime(most_recent_article-timedelta(15), utc=True)\n",
    "mask = (liverss_and_coms_df['published'] > recent_date)  # & (total_df['is_com_or_sub'] == 'sub') -> not yet implemented\n",
    "recent_articles = liverss_and_coms_df.loc[mask]\n",
    "recent_articles_id = recent_articles['id'].tolist()\n",
    "print(recent_articles_id[:3])\n",
    "print(len(recent_articles_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more LOAD MORE RESULTS button to be clicked\n",
      "1 comments found\n"
     ]
    }
   ],
   "source": [
    "# Configure the Chrome webdriver\n",
    "driver = webdriver.Chrome('/users/michaellappert/Downloads/chromedriver') # Needs to be adjusted client independently\n",
    "\n",
    "# get the commenents of those articles\n",
    "new_coms_df = pd.DataFrame(columns = ['author', 'published', 'summary', 'id', 'is_com_or_sub', 'link'])\n",
    "\n",
    "for Id, link in zip(recent_articles['id'], recent_articles['link']):\n",
    "    print(link)\n",
    "    driver.get(link)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"button.show-more-bar__child.show-more-bar__child--grow.js-show-more-button.button--white.button--mixed-case.button--link-text-color.button--no-spacing.button--no-text-on-mobile\"))).click()\n",
    "            print(\"LOAD MORE RESULTS button clicked\")\n",
    "        except TimeoutException:\n",
    "            try:\n",
    "                WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME, \"comment__link.comment__link--expand\"))).click()\n",
    "            except TimeoutException:  \n",
    "                print(\"No more LOAD MORE RESULTS button to be clicked\")\n",
    "                break\n",
    "            \n",
    "    # get authors\n",
    "    authors = driver.find_elements_by_class_name(\"comment__user\")\n",
    "    authors_list = [author.get_attribute('innerHTML') for author in authors]\n",
    "    authors_list = [item.replace('Kommentar von','')\\\n",
    "                    .replace('\\n','')\\\n",
    "                    .replace('<span class=\"h-offscreen\"> ','')\\\n",
    "                    .replace('<span>', '')\\\n",
    "                    .replace('</span>', '')\\\n",
    "                    .replace('&nbsp;', '')\\\n",
    "                    .replace('  ', '') for item in authors_list]\n",
    "    #authors_list = [item for item in authors_list if item != '']\n",
    "    # print('authors:', authors_list[:5])\n",
    "\n",
    "\n",
    "    # get publication dates\n",
    "    published_date = driver.find_elements_by_class_name(\"comment__time\")\n",
    "    date_list = [pd.to_datetime(date.get_attribute('data-timestamp'), unit='s', utc=True) for date in published_date]\n",
    "    # print('date_list: ', date_list[:5])\n",
    "    \n",
    "    # get text bodys\n",
    "    bodys = driver.find_elements_by_class_name(\"comment__content\")\n",
    "    body_list = [body.get_attribute('innerHTML') for body in bodys]\n",
    "    body_list = [item.replace('<br>', '').replace('\\n','') for item in body_list]\n",
    "    for item in body_list:\n",
    "        if item == 'Beliebteste Kommentare werden geladen.':\n",
    "            body_list.remove(item)\n",
    "    # print('body_list:\\n', body_list[:5])\n",
    "    # print(body_list[:5])\n",
    "\n",
    "    # assert to make sure that we have all the data for every com\n",
    "    # may fail if the webdriver didn't wait for the page to load\n",
    "    assert len(body_list) == len(date_list) == len(authors_list), f\"Assertion problem, probably due to page parsing before page is fully loaded : bodys:{body_list} dates:{date_list} authors:{authors_list}\"\n",
    "    \n",
    "    temp_df = pd.DataFrame(list(zip(authors_list, date_list, body_list)), columns =['author', 'published', 'summary'])\n",
    "    temp_df['id'] = Id\n",
    "    temp_df['com_or_sub'] = 'com'\n",
    "    temp_df['link'] = link\n",
    "    temp_df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "    print(f'{len(temp_df)} comments found')\n",
    "    if len(temp_df) > 0:\n",
    "        new_coms_df = pd.concat([new_coms_df, temp_df], ignore_index=True, axis=0)\n",
    "\n",
    "new_coms_df.head()\n",
    "driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for newest coms and getting newest new_last_check_coms\n",
    "# srf_article_coms_df = new_coms_df[new_coms_df['date'] > last_coms_check] -> not yet implemented\n",
    "srf_article_coms_df = new_coms_df\n",
    "\n",
    "if len(srf_article_coms_df) > 0:\n",
    "    new_last_check_coms = srf_article_coms_df['published'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving generated data to raw data folder\n",
    "srf_article_coms_df.to_csv(r'~/Desktop/SocialMediaMonitor/data/raw/liverss_comments_srf.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7fa4ad54c13063743e0b795d31fc2de87e75f8ad2886ded3b88bc7649886d47"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('socialMediaMonitor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
